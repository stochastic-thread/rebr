{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-11ae96b69d92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_pickle_dump\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_feedparser_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Config'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Queries arxiv API and downloads papers (the query is a parameter).\n",
    "The script is intended to enrich an existing database pickle (by default db.p),\n",
    "so this file will be loaded first, and then new results will be added to it.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import urllib.request\n",
    "import feedparser\n",
    "\n",
    "from utils import Config, safe_pickle_dump\n",
    "\n",
    "def encode_feedparser_dict(d):\n",
    "  \"\"\" \n",
    "  helper function to get rid of feedparser bs with a deep copy. \n",
    "  I hate when libs wrap simple things in their own classes.\n",
    "  \"\"\"\n",
    "  if isinstance(d, feedparser.FeedParserDict) or isinstance(d, dict):\n",
    "    j = {}\n",
    "    for k in d.keys():\n",
    "      j[k] = encode_feedparser_dict(d[k])\n",
    "    return j\n",
    "  elif isinstance(d, list):\n",
    "    l = []\n",
    "    for k in d:\n",
    "      l.append(encode_feedparser_dict(k))\n",
    "    return l\n",
    "  else:\n",
    "    return d\n",
    "\n",
    "def parse_arxiv_url(url):\n",
    "  \"\"\" \n",
    "  examples is http://arxiv.org/abs/1512.08756v2\n",
    "  we want to extract the raw id and the version\n",
    "  \"\"\"\n",
    "  ix = url.rfind('/')\n",
    "  idversion = url[ix+1:] # extract just the id (and the version)\n",
    "  parts = idversion.split('v')\n",
    "  assert len(parts) == 2, 'error parsing url ' + url\n",
    "  return parts[0], int(parts[1])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "  # parse input arguments\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--search-query', type=str,\n",
    "                      default='cat:cs.CV+OR+cat:cs.AI+OR+cat:cs.LG+OR+cat:cs.CL+OR+cat:cs.NE+OR+cat:stat.ML',\n",
    "                      help='query used for arxiv API. See http://arxiv.org/help/api/user-manual#detailed_examples')\n",
    "  parser.add_argument('--start-index', type=int, default=0, help='0 = most recent API result')\n",
    "  parser.add_argument('--max-index', type=int, default=10000, help='upper bound on paper index we will fetch')\n",
    "  parser.add_argument('--results-per-iteration', type=int, default=100, help='passed to arxiv API')\n",
    "  parser.add_argument('--wait-time', type=float, default=5.0, help='lets be gentle to arxiv API (in number of seconds)')\n",
    "  parser.add_argument('--break-on-no-added', type=int, default=1, help='break out early if all returned query papers are already in db? 1=yes, 0=no')\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  # misc hardcoded variables\n",
    "  base_url = 'http://export.arxiv.org/api/query?' # base api query url\n",
    "  print('Searching arXiv for %s' % (args.search_query, ))\n",
    "\n",
    "  # lets load the existing database to memory\n",
    "  try:\n",
    "    db = pickle.load(open(Config.db_path, 'rb'))\n",
    "  except Exception as e:\n",
    "    print('error loading existing database:')\n",
    "    print(e)\n",
    "    print('starting from an empty database')\n",
    "    db = {}\n",
    "\n",
    "  # -----------------------------------------------------------------------------\n",
    "  # main loop where we fetch the new results\n",
    "  print('database has %d entries at start' % (len(db), ))\n",
    "  num_added_total = 0\n",
    "  for i in range(args.start_index, args.max_index, args.results_per_iteration):\n",
    "\n",
    "    print(\"Results %i - %i\" % (i,i+args.results_per_iteration))\n",
    "    query = 'search_query=%s&sortBy=lastUpdatedDate&start=%i&max_results=%i' % (args.search_query,\n",
    "                                                         i, args.results_per_iteration)\n",
    "    with urllib.request.urlopen(base_url+query) as url:\n",
    "      response = url.read()\n",
    "    parse = feedparser.parse(response)\n",
    "    num_added = 0\n",
    "    num_skipped = 0\n",
    "    for e in parse.entries:\n",
    "\n",
    "      j = encode_feedparser_dict(e)\n",
    "\n",
    "      # extract just the raw arxiv id and version for this paper\n",
    "      rawid, version = parse_arxiv_url(j['id'])\n",
    "      j['_rawid'] = rawid\n",
    "      j['_version'] = version\n",
    "\n",
    "      # add to our database if we didn't have it before, or if this is a new version\n",
    "      if not rawid in db or j['_version'] > db[rawid]['_version']:\n",
    "        db[rawid] = j\n",
    "        print('Updated %s added %s' % (j['updated'].encode('utf-8'), j['title'].encode('utf-8')))\n",
    "        num_added += 1\n",
    "        num_added_total += 1\n",
    "      else:\n",
    "        num_skipped += 1\n",
    "\n",
    "    # print some information\n",
    "    print('Added %d papers, already had %d.' % (num_added, num_skipped))\n",
    "\n",
    "    if len(parse.entries) == 0:\n",
    "      print('Received no results from arxiv. Rate limiting? Exiting. Restart later maybe.')\n",
    "      print(response)\n",
    "      break\n",
    "\n",
    "    if num_added == 0 and args.break_on_no_added == 1:\n",
    "      print('No new papers were added. Assuming no new papers exist. Exiting.')\n",
    "      break\n",
    "\n",
    "    print('Sleeping for %i seconds' % (args.wait_time , ))\n",
    "    time.sleep(args.wait_time + random.uniform(0, 3))\n",
    "\n",
    "  # save the database before we quit, if we found anything new\n",
    "  if num_added_total > 0:\n",
    "    print('Saving database with %d papers to %s' % (len(db), Config.db_path))\n",
    "    safe_pickle_dump(db, Config.db_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Config in /Users/arthurcolle/anaconda3/lib/python3.6/site-packages (0.4.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
